{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#算法实现——自定义Kmeans类\" data-toc-modified-id=\"算法实现——自定义Kmeans类-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>算法实现——自定义Kmeans类</a></span><ul class=\"toc-item\"><li><span><a href=\"#初始聚类中心优化的-k-means-算法-$\\renewcommand{\\baselinestretch}{1.5}$\" data-toc-modified-id=\"初始聚类中心优化的-k-means-算法-$\\renewcommand{\\baselinestretch}{1.5}$-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>初始聚类中心优化的 k-means 算法 $\\renewcommand{\\baselinestretch}{1.5}$</a></span><ul class=\"toc-item\"><li><span><a href=\"#一般-k-means-算法及其缺陷-$\\renewcommand{\\baselinestretch}{1.5}$\" data-toc-modified-id=\"一般-k-means-算法及其缺陷-$\\renewcommand{\\baselinestretch}{1.5}$-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>一般 k-means 算法及其缺陷 $\\renewcommand{\\baselinestretch}{1.5}$</a></span></li><li><span><a href=\"#优化初始聚类中心的-k-means-算法-$\\renewcommand{\\baselinestretch}{1.5}$\" data-toc-modified-id=\"优化初始聚类中心的-k-means-算法-$\\renewcommand{\\baselinestretch}{1.5}$-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>优化初始聚类中心的 k-means 算法 $\\renewcommand{\\baselinestretch}{1.5}$</a></span></li><li><span><a href=\"#自定义Kmeans类介绍-$\\renewcommand{\\baselinestretch}{1.5}$\" data-toc-modified-id=\"自定义Kmeans类介绍-$\\renewcommand{\\baselinestretch}{1.5}$-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>自定义Kmeans类介绍 $\\renewcommand{\\baselinestretch}{1.5}$</a></span></li></ul></li><li><span><a href=\"#K-means-算法最佳聚类数确定方法-$\\renewcommand{\\baselinestretch}{1.5}$\" data-toc-modified-id=\"K-means-算法最佳聚类数确定方法-$\\renewcommand{\\baselinestretch}{1.5}$-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>K-means 算法最佳聚类数确定方法 $\\renewcommand{\\baselinestretch}{1.5}$</a></span><ul class=\"toc-item\"><li><span><a href=\"#类内距离和类间距离的定义和计算公式-$\\renewcommand{\\baselinestretch}{1.5}$\" data-toc-modified-id=\"类内距离和类间距离的定义和计算公式-$\\renewcommand{\\baselinestretch}{1.5}$-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>类内距离和类间距离的定义和计算公式 $\\renewcommand{\\baselinestretch}{1.5}$</a></span></li><li><span><a href=\"#BWP指标与最佳聚类数确定-$\\renewcommand{\\baselinestretch}{1.5}$\" data-toc-modified-id=\"BWP指标与最佳聚类数确定-$\\renewcommand{\\baselinestretch}{1.5}$-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>BWP指标与最佳聚类数确定 $\\renewcommand{\\baselinestretch}{1.5}$</a></span></li></ul></li><li><span><a href=\"#k-means-与-k-medoids-$\\renewcommand{\\baselinestretch}{1.5}$\" data-toc-modified-id=\"k-means-与-k-medoids-$\\renewcommand{\\baselinestretch}{1.5}$-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>k-means 与 k-medoids $\\renewcommand{\\baselinestretch}{1.5}$</a></span></li></ul></li><li><span><a href=\"#测试\" data-toc-modified-id=\"测试-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>测试</a></span><ul class=\"toc-item\"><li><span><a href=\"#用模拟数据测试Kmeans类并选出最优k值-$\\renewcommand{\\baselinestretch}{1.5}$\" data-toc-modified-id=\"用模拟数据测试Kmeans类并选出最优k值-$\\renewcommand{\\baselinestretch}{1.5}$-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>用模拟数据测试Kmeans类并选出最优k值 $\\renewcommand{\\baselinestretch}{1.5}$</a></span></li><li><span><a href=\"#用真实数据测试Kmeans类——对比k-means,-k-medoids-$\\renewcommand{\\baselinestretch}{1.5}$\" data-toc-modified-id=\"用真实数据测试Kmeans类——对比k-means,-k-medoids-$\\renewcommand{\\baselinestretch}{1.5}$-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>用真实数据测试Kmeans类——对比k-means, k-medoids $\\renewcommand{\\baselinestretch}{1.5}$</a></span></li><li><span><a href=\"#自定义Kmeans类与sklearn库中Kmeans方法准确度比较-$\\renewcommand{\\baselinestretch}{1.5}$\" data-toc-modified-id=\"自定义Kmeans类与sklearn库中Kmeans方法准确度比较-$\\renewcommand{\\baselinestretch}{1.5}$-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>自定义Kmeans类与sklearn库中Kmeans方法准确度比较 $\\renewcommand{\\baselinestretch}{1.5}$</a></span></li></ul></li><li><span><a href=\"#总结-$\\renewcommand{\\baselinestretch}{1.5}$\" data-toc-modified-id=\"总结-$\\renewcommand{\\baselinestretch}{1.5}$-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>总结 $\\renewcommand{\\baselinestretch}{1.5}$</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>k-means算法的改进与测试——基于矩阵运算的实现<span class=\"tocSkip\"></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$徐菡\n",
    "2017310719\n",
    "\n",
    "$\\quad$金融实验班17\n",
    "\n",
    "$\\quad$2019/5/23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法实现——自定义Kmeans类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始聚类中心优化的 k-means 算法 $\\renewcommand{\\baselinestretch}{1.5}$\n",
    "——引自 袁方,周志勇,宋鑫. 初始聚类中心优化的k-means算法 [J]. 计算机工程, 2007(33): 65-66\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一般 k-means 算法及其缺陷 $\\renewcommand{\\baselinestretch}{1.5}$\n",
    "\n",
    "- 一般的k-means 算法描述如下：\n",
    "\n",
    "$\\quad$输入：聚类个数k以及包含n个数据对象的数据集；\n",
    "\n",
    "$\\quad$输出：满足目标函数值最小的k个聚类。\n",
    "\n",
    "$\\quad$算法流程：\n",
    "\n",
    "（1）从n个数据对象中任意选择k个对象作为初始聚类中心；\n",
    "\n",
    "（2）循环下述流程(3)到(4)，直到目标函数J 取值不再变化；\n",
    "\n",
    "（3）根据每个聚类对象的均值(中心对象)，计算每个对象与这些中心对象的距离，并且根据最小距离重新对相应对象进行划分；\n",
    "\n",
    "（4）重新计算每个聚类的均值（中心对象）。\n",
    "\n",
    "- 传统的 k-means 算法对初始聚类中心敏感，不同的初始中心往往对应着不同的聚类结果。因此我们想找到一组能反映数据分布特征的数据对象作为初始聚类中心，也就是改进上述算法中的第(1)步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化初始聚类中心的 k-means 算法 $\\renewcommand{\\baselinestretch}{1.5}$\n",
    "- 为了避免取到噪声点，取相互距离最远的k个处于高密度区域的点作为初始聚类中心。为了计算数据对象$x_i$所处区域的密度，定义一个密度参数：以$x_i$为中心，包含常数Minpts 个数据对象的半径称之为对象$x_i$的密度参数，用ε表示。ε越大，说明数据对象所处区域的数据密度越低。反之，ε越小，说明数据对象所处区域的数据密度越高。通过计算每个数据对象的密度参数，就可以发现处于高密度区域的点，从而得到一个高密度点集合D。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优化初始聚类中心的 k-means 算法描述：$\\renewcommand{\\baselinestretch}{1.5}$\n",
    "\n",
    "$\\quad$输入：聚类个数 k 以及包含n 个数据对象的数据集；\n",
    "\n",
    "$\\quad$输出：满足目标函数值最小的 k 个聚类。\n",
    "\n",
    "（1）计算任意两个数据对象间的距离d($x_i$, $x_j$)；\n",
    "\n",
    "（2）计算每个数据对象的密度参数，把处于低密度区域的点删除，得到处于高密度区域的数据对象的集合D；\n",
    "\n",
    "（3）把处于最高密度区域的数据对象作为第1个中心$z_1$；\n",
    "\n",
    "（4）把$z_1$距离最远的数据对象作为第2 个初始中心$z_2$，$z_2$∈D；\n",
    "\n",
    "（5）令$z_3$为满足 $max(min(d(x_i,z_1), d(x_i,z_2)), i =1,2,…,n$ 的数据对象$x_i$, $z_3$∈D；\n",
    "\n",
    "（6） 令$z_4$为 满 足 $max(min(d(x_i,z_1), d(x_i,z_2), d(x_i, z_3)), i =1,2,…,n$ 的$x_i$, $z_4$∈D；\n",
    "\n",
    "…\n",
    "\n",
    "（7）令$z_k$为满足$max(min(d(x_i,z_j), d(x_i,z_2)), i=1,2,…,n, j=1,2,…,k-1$ 的$x_i$, $z_k$∈D；\n",
    "\n",
    "（8）从这k个聚类中心出发，应用k-means聚类算法，得到聚类结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自定义Kmeans类介绍 $\\renewcommand{\\baselinestretch}{1.5}$\n",
    "\n",
    "（1）参数\n",
    "\n",
    "- minpts：用以xi为中心，包含常数minpts个数据对象的半径来衡量xi的密度\n",
    "\n",
    "- p：初始中心备选点的个数\n",
    "\n",
    "- k：聚类个数\n",
    "\n",
    "- center_method：计算聚类中心的方法，默认值为\"mean\"\n",
    "\n",
    "（2）方法\n",
    "\n",
    "- init_density()：初始化每个对象的密度\n",
    "\n",
    "- init_centroid()：初始化中心\n",
    "\n",
    "- k_means()：执行k-means算法\n",
    "\n",
    "- plot_cluster()：在二维平面画出聚类结果\n",
    "\n",
    "- cal_BWP()：计算平均BWP指标（在1.2节中详述）\n",
    "\n",
    "- cal_all()：将上述方法一次性执行\n",
    "\n",
    "- new_assign()：对于新数据进行分类（将新数据划分到已经聚好的类中）\n",
    "\n",
    "（3）属性\n",
    "\n",
    "- data：待聚类的数据\n",
    "\n",
    "- n：数据对象的个数\n",
    "\n",
    "- dim：数据维数，即特征个数（一般用k表示，但在本类中k表示聚类个数，因此用dim代替）\n",
    "\n",
    "- distance：对象两两之间的距离的矩阵，distance[i][j]表示第i和对象和第j个对象之间的距离\n",
    "\n",
    "- density：每个对象的密度\n",
    "\n",
    "- centroid：每个类的中心坐标，初始值为由上述方法选出的k个点\n",
    "\n",
    "- J：判断迭代是否结束的目标函数，若两次相邻迭代后J保持不变，则停止迭代\n",
    "\n",
    "- point_cluster_dist：每个对象到k个类中心的距离\n",
    "\n",
    "- point_cluster_assign：每个对象距离最近的中心在centroid数组中的索引\n",
    "\n",
    "- cluster：列表存放每个类中的对象在data中的索引\n",
    "\n",
    "- avg_BWP：当前参数下的平均BWP指标值，用于选择最优参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans类，用于执行上述算法步骤\n",
    "class Kmeans(object):\n",
    "    def __init__(self, data, minpts, p, k, center_method=\"mean\"):\n",
    "        self.data = data\n",
    "        self.minpts = minpts  # 用以xi为中心，包含常数minpts个数据对象的半径来衡量xi的密度\n",
    "        self.n = data.shape[0]  # 数据对象的个数\n",
    "        self.dim = data.shape[1]  # 每个对象的维度\n",
    "        self.p = p  # 取p个密度较大的点作为初始中心的备选点\n",
    "        self.k = k  # 聚成的类的个数\n",
    "        self.center_method = center_method  # 计算中心的方法\n",
    "        self.distance = mm_EuDistance(data, data)  # 对象两两之间的距离的矩阵，distance[i][j]表示第i和对象和第j个对象之间的距离\n",
    "        self.density = np.zeros(self.n)  # 每个对象的密度\n",
    "        self.centroid = np.zeros([k, self.dim])  # 每个类的中心坐标，初始值为由上述方法选出的k个点\n",
    "        self.J = 0  # 判断迭代是否结束的目标函数，若两次相邻迭代后J保持不变，则停止迭代\n",
    "        self.point_cluster_dist = np.zeros([self.n, k])  # 每个对象到k个类中心的距离\n",
    "        self.point_cluster_assign = np.zeros(self.n)  # 每个对象距离最近的中心在centroid数组中的索引\n",
    "        self.cluster = []  # cluster列表存放每个类中的对象在data中的索引\n",
    "        self.avg_BWP = 0  # 当前参数下的平均BWP指标值，用于选择最优参数\n",
    "\n",
    "    # 初始化每个对象的密度\n",
    "    def init_density(self):\n",
    "        for i in range(self.n):\n",
    "            nearest = self.distance[i].argsort()[:self.minpts + 1]  # 每个对象距离最近的minpts个点（包含对象自身）\n",
    "            self.density[i] = self.distance[nearest][:, nearest].sum()  # 这minpts个对象的两两距离之和，density越小，密度越大\n",
    "\n",
    "    # 初始化中心\n",
    "    def init_centroid(self):\n",
    "        high = self.density.argsort()[:self.p]  # 取density中前p个对象，即密度最大的p个对象\n",
    "        used = [high[0]]  # used列表表示已经选出的作为初始中心的对象\n",
    "        unused = high[1:]  # 剩下的没有作为初始中心的对象\n",
    "        pick = [0] * self.k\n",
    "        # 选剩下的k-1个初始中心\n",
    "        for i in range(self.k - 1):\n",
    "            pick[i] = np.min(self.distance[used][:, unused])\n",
    "            used.append(np.where(self.distance == np.max(pick[i]))[0].tolist()[0])\n",
    "            used.append(np.where(self.distance == np.max(pick[i]))[0].tolist()[1])\n",
    "            used = np.unique(used)\n",
    "            used = used.tolist()\n",
    "            unused = list(filter(lambda x: x not in used, high))\n",
    "        self.centroid = self.data[used]\n",
    "\n",
    "    def k_means(self):\n",
    "        iteration = 0  # 记录迭代次数\n",
    "        j_changed = True\n",
    "        while j_changed:\n",
    "            # 根据每个类对象的均值(中心对象)，计算每个对象与这些中心对象的距离\n",
    "            self.point_cluster_dist = mm_EuDistance(self.data, self.centroid)\n",
    "            # 根据最小距离重新对相应对象进行划分\n",
    "            self.point_cluster_assign = np.argmin(self.point_cluster_dist, axis=1)\n",
    "            # 重新计算每个类的均值（中心对象）\n",
    "            # centroid[0] = cal_mean(np.where(point_cluster_assign==0))\n",
    "            self.centroid = np.array(\n",
    "                list(map(lambda j: cal_center(self.data[np.where(self.point_cluster_assign == j)], self.center_method),\n",
    "                         list(range(self.k)))))\n",
    "            # print(centroid)\n",
    "            iteration += 1\n",
    "            # 判断目标函数J是否改变，并更新J\n",
    "            j_new = np.sum(self.point_cluster_dist)\n",
    "            if self.J == j_new:\n",
    "                j_changed = False\n",
    "            self.J = j_new\n",
    "        print(\"迭代次数为：\")\n",
    "        print(iteration)\n",
    "        return self.centroid\n",
    "\n",
    "    # 只能画出二维数据\n",
    "    def plot_cluster(self):\n",
    "        if self.dim != 2:\n",
    "            print(\"对不起，本模块无法画出三维及以上数据!\")\n",
    "            return 1\n",
    "        mark = ['or', 'ob', 'og', 'ok', '^r', '+r', 'sr', 'dr', '<r', 'pr']\n",
    "        if self.k > len(mark):\n",
    "            print(\"对不起，本模块无法画出十一及以上类别\")\n",
    "            return 1\n",
    "        # data_sort = self.data.append(self.point_cluster_assign, axis=1)\n",
    "        for i in range(self.n):\n",
    "            markIndex = int(self.point_cluster_assign[i])  # 为样本指定颜色\n",
    "            plt.plot(self.data[i][0], self.data[i][1], mark[markIndex])\n",
    "        mark = ['Dr', 'Db', 'Dg', 'Dk', '^b', '+b', 'sb', 'db', '<b', 'pb']\n",
    "        # 画出中心点\n",
    "        for i in range(self.k):\n",
    "            plt.plot(self.centroid[i][0], self.centroid[i][1], mark[i], markersize=12)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### K-means 算法最佳聚类数确定方法 $\\renewcommand{\\baselinestretch}{1.5}$\n",
    "——引自 周世兵,徐振源,唐旭清. K-means 算法最佳聚类数确定方法 [J]. 计算机应用, 2010, 30(8): 1995-1998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 类内距离和类间距离的定义和计算公式 $\\renewcommand{\\baselinestretch}{1.5}$\n",
    "- 一般来说，一个好的聚类划分应尽可能反映数据集的内在结构，使类内样本尽可能相似，类间样本尽可能不相似。从距离测度考虑，就是使类内距离极小化而类间距离最大化的聚类是最优聚类。本文设计了一种新的聚类有效性指标，该指标可以对K-means 算法的聚类结果进行评估并可用来确定最佳聚类数。\n",
    "\n",
    "- 定义1 令 K = {X，R}为聚类空间，其中 X = {$x_1,x_2,...,x_n$}，假设n个样本对象被聚类为c类，定义第j类的第i个样本的最小类间距离b(j,i)为该样本到其他每个类中样本平均距离的最小值：\n",
    "\n",
    "\n",
    "$$b(j,i) =\\min \\limits_{1\\le k\\le c, k\\neq j}(\\frac{1}{n_k}\\sum_{p = 1}^{n_k}\\|x_p^{(k)} - x_i^{(j)}\\|^2)$$\n",
    "\n",
    "$\\quad$其中: k和j表示类标，$x_i^{(j)}$表示第j类的第i个样本，$x_p^{(k)}$表示第k类的第p个样本，$n_k$表示第k 类中的样本个数，$\\|.\\|^2$表示平方欧氏距离。\n",
    "\n",
    "- 定义2 令K = {X，R}为聚类空间，其中X = {$x_1,x_2,...,x_n$}，假设n个样本对象被聚类为c类，定义第j类的第i个样本的类内距离w(j,i)为该样本到第j 类中其他所有样本的平均距离，即:\n",
    "\n",
    "$$w(j,i) =\\frac{1}{n_j-1}\\sum_{q = 1, q \\neq i}^{n_j}\\|x_q^{(j)} - x_i^{(j)}\\|^2$$\n",
    "\n",
    "$\\quad$其中: $x_q^{(j)}$表示第j 类中的第q 个样本，并且q≠i, $n_j$表示第j类中的样本个数。实际使用中，无需保证q ≠ i，因为q = i时，欧氏距离为0，并不影响算法的正确性。\n",
    "\n",
    "- 定义3 令K = {X，R} 为聚类空间，其中X = {$x_1,x_2,...,x_n$}，假设n个样本对象被聚类为c类，定义第j类的第i个样本的聚类距离baw(j,i)为该样本的最小类间距离和类内距离之和，即:\n",
    "\n",
    "$$baw(j,i)=b(j,i)+w(j,i)=\\min \\limits_{1\\le k\\le c, k\\neq j}(\\frac{1}{n_k}\\sum_{p = 1}^{n_k}\\|x_p^{(k)} - x_i^{(j)}\\|^2)+\\frac{1}{n_j-1}\\sum_{q = 1, q \\neq i}^{n_j}\\|x_q^{(j)} - x_i^{(j)}\\|^2$$\n",
    "\n",
    "- 定义4 令K = {X，R} 为聚类空间，其中X =  {$x_1,x_2,...,x_n$}，假设n个样本对象被聚类为c类，定义第j类的第i个样本的聚类离差距离bsw(j,i)为该样本的最小类间距离和类内距离之差，即:\n",
    "\n",
    "$$bsw(j,i)=b(j,i)+w(j,i)=\\min \\limits_{1\\le k\\le c, k\\neq j}(\\frac{1}{n_k}\\sum_{p = 1}^{n_k}\\|x_p^{(k)} - x_i^{(j)}\\|^2)-\\frac{1}{n_j-1}\\sum_{q = 1, q \\neq i}^{n_j}\\|x_q^{(j)} - x_i^{(j)}\\|^2$$\n",
    "\n",
    "\n",
    "- 定义5 令K = {X，R} 为聚类空间，其中X = {$x_1,x_2,...,x_n$}，假设n个样本对象被聚类为c类，定义第j类的第i个样本的类间类内划分( Between-Within Proportion，BWP)指标BWP(j,i)为该样本的聚类离差距离和聚类距离的比值，即:\n",
    "\n",
    "\n",
    "$$ BWP=\\frac{\\min \\limits_{1\\le k\\le c, k\\neq j}(\\frac{1}{n_k}\\sum_{p = 1}^{n_k}\\|x_p^{(k)} - x_i^{(j)}\\|^2) - \\frac{1}{n_j-1}\\sum_{q = 1, q \\neq i}^{n_j}\\|x_q^{(j)} - x_i^{(j)}\\|^2}{\\min \\limits_{1\\le k\\le c, k\\neq j}(\\frac{1}{n_k}\\sum_{p = 1}^{n_k}\\|x_p^{(k)} - x_i^{(j)}\\|^2) + \\frac{1}{n_j-1}\\sum_{q = 1, q \\neq i}^{n_j}\\|x_q^{(j)} - x_i^{(j)}\\|^2} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BWP指标反映了单个样本的聚类有效性情况，BWP指标值越大，说明单个样本的聚类效果越好。我们通过求某个数据集中所有样本的BWP指标值的平均值，来分析该数据集的聚类效果。显然，平均值越大，说明该数据集的聚类效果越好，其最大值所对应的聚类数是最佳聚类数。由此我们得到如下公式，其中$avg_BWP(k)$表示数据集聚成k类时的平均BWP指标值，$k_opt$表示最佳聚类数。\n",
    "\n",
    "$$avg_{BWP}(k) = \\frac{1}{n}\\sum_{j = 1}^{k}\\sum_{i = 1}^{n_j} BWP(j,i)$$\n",
    "\n",
    "$$k_{opt} = \\mathop{\\arg\\max}_{{2\\le k\\le n}}\\{ avg_{BWP}(k) \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BWP指标与最佳聚类数确定 $\\renewcommand{\\baselinestretch}{1.5}$\n",
    "\n",
    " - 本文结合 k-means 算法以及式(5) 定义的 BWP 聚类有效性指标，提出一种新的分析聚类效果，确定最佳聚类数的算法。算法归纳如下。\n",
    "\n",
    "$\\quad$（1）选择聚类数的搜索范围$[k_{min}，k_{max}]$。\n",
    "\n",
    "$\\quad$（2）从$k_{min}$循环至$k_{max}$:\n",
    "\n",
    "$\\qquad$① 调用K-means算法;\n",
    "\n",
    "$\\qquad$② 利用式(5) 计算单个样本的BWP 指标值;\n",
    "\n",
    "$\\qquad$③ 利用式(6) 计算平均BWP 指标值。\n",
    "\n",
    "$\\quad$（3）利用式(7) 计算最佳聚类数。\n",
    "\n",
    "$\\quad$（4）输出最佳聚类数、有效性指标值和聚类结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义在Kmeans类内的函数cal_BWP用于计算上述BWP指标\n",
    "    # 计算b(i,j), w(i,j)以及BWP(i,j)，并求出该类数k下的平均BWP（avg_BWP)\n",
    "    def cal_BWP(self):\n",
    "        cluster = list(\n",
    "            map(lambda j: np.array(list(range(self.n)))[np.where(self.point_cluster_assign == j)], list(range(self.k))))\n",
    "        # 定义第j类的第i个样本的最小类间距离b(j，i)：该样本到其他每个类中样本平均距离的最小值\n",
    "        self.cluster = cluster\n",
    "        b_mat = list(range(self.k))\n",
    "        for j in range(self.k):\n",
    "            other_list = list(range(j)) + list(range(j + 1, self.k))\n",
    "            b_mat[j] = np.min(\n",
    "                np.array(list(map(lambda x: np.mean(self.distance[cluster[j]][:, cluster[x]], axis=1), other_list))),\n",
    "                axis=0)\n",
    "        # 第j类的第i个样本的类内距离w(j，i)：该样本到第j类中其他所有样本的平均距离\n",
    "        w_mat = list(range(self.k))\n",
    "        for j in range(self.k):\n",
    "            w_mat[j] = np.mean(self.distance[cluster[j]][:, cluster[j]], axis=1)\n",
    "        baw = np.array(b_mat) + np.array(w_mat)\n",
    "        bsw = np.array(b_mat) - np.array(w_mat)\n",
    "        BWP = bsw / baw\n",
    "        self.avg_BWP = np.array(list(map(lambda i: np.mean(BWP[i]), list(range(self.k))))).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义类外函数，循环求出最佳聚类数k\n",
    "def cal_best_k(data, k_list, minpt = 10, p = 60):\n",
    "    BWP_list = []\n",
    "    for k in k_list:\n",
    "        km = Kmeans(data, minpt, p, k)\n",
    "        km.cal_all()\n",
    "        BWP_list.append(km.avg_BWP)\n",
    "    print(BWP_list)\n",
    "    return k_list[np.argmax(np.array(BWP_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means 与 k-medoids $\\renewcommand{\\baselinestretch}{1.5}$\n",
    "\n",
    "$\\quad$参考http://blog.sina.com.cn/s/blog_5fc375650100jdec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$k-means通过计算一类记录的均值来代表该类，但是受异常值或极端值的影响比较大，而另外一种算法k-medoids可以解决这个问题。\n",
    "\n",
    "$\\quad$k-medoids算法k-means十分类似，二者区别在于中心点的选取，在k-means中，我们将中心点取为当前cluster中所有数据点的平均值，而在k-medoids算法中，我们将从当前cluster中选取这样一个点——它到其他所有（当前cluster中的）点的距离之和最小——作为中心点。\n",
    "\n",
    "$\\quad$由于k-medoids算法的流程和k-means相同，差别仅在于中心点计算的方式，因此我们可以在Kmeans类中将中心点计算方法作为参数输入，并添加一种计算中心点的方式，就可以使Kmeans类同时具有执行k-medoids算法的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义类外函数cal_center\n",
    "# 在K-means中，我们将中心点取为当前cluster中所有数据点的平均值，\n",
    "# 在K-medoids算法中，我们将从当前cluster中选取这样一个点——它到其他所有（当前cluster中的）点的距离之和最小——作为中心点。\n",
    "def cal_center(X, method = \"mean\"):\n",
    "    if method == \"mean\":\n",
    "        center = np.mean(X, axis=0)\n",
    "    if method == \"medoid\":\n",
    "        distance_tmp = mm_EuDistance(X,X)\n",
    "        center = X[np.argmin(np.sum(distance_tmp, axis=0), axis=0)]\n",
    "    return center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用模拟数据测试Kmeans类并选出最优k值 $\\renewcommand{\\baselinestretch}{1.5}$\n",
    "\n",
    "将上述Kmeans类以及类外函数写入python文件，在此调用，以测试Kmeans类的功能，探究最优k值的规律"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入Kmeans类\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir(\"D:/1s/数据挖掘/1大作业/聚类/k_means\")\n",
    "from Kmeans import Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数为：\n",
      "5\n",
      "迭代次数为：\n",
      "7\n",
      "迭代次数为：\n",
      "10\n",
      "迭代次数为：\n",
      "12\n",
      "迭代次数为：\n",
      "16\n",
      "迭代次数为：\n",
      "12\n",
      "迭代次数为：\n",
      "7\n",
      "迭代次数为：\n",
      "8\n",
      "[0.14640027023289875, 0.14829840872416228, 0.18771109667504968, 0.17724432191108216, 0.15743637945540942, 0.16124675836834815, 0.15677764865782978, 0.16777556346042902]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# 随机生成100个七维数据，k的取值范围从2到\\sqrt{100}=10\n",
    "np.random.seed(5)\n",
    "data = np.random.randn(100,3)\n",
    "k_list = list(range(2,10))\n",
    "best_k = cal_best_k(data, k_list)\n",
    "print(best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$从以上结果可以看出，随着k的增大，BWP指标先增大后减小，使得BWP取到最大值的k值为4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数为：\n",
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 用二维数据画出聚类结果（取k=4）\n",
    "np.random.seed(7)\n",
    "data2 = np.random.randn(100,2)\n",
    "k2 = Kmeans(data2, 10, 25, 4)\n",
    "k2.cal_all()\n",
    "k2.plot_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用真实数据测试Kmeans类——对比k-means, k-medoids $\\renewcommand{\\baselinestretch}{1.5}$\n",
    "\n",
    "$\\quad$在这个部分，我选用了UCI数据库中的经典数据Iris数据http://archive.ics.uci.edu/ml/datasets/Iris 来进行测试，将数据集按照2:1的比例分为训练集和测试集，将训练集输入Kmeans类中，算出聚类中心，然后利用这些聚类中心对测试集数据进行类别预测。分别对k-means和k-medoids方法进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"D:/1s/数据挖掘/1大作业/聚类/data/iris.csv\")\n",
    "# 生成乱序index来打乱数据顺序，并按照2:1的比例分为训练系和测试集\n",
    "np.random.seed(7)\n",
    "index1 = np.arange(len(iris))  # 生成下标\n",
    "np.random.shuffle(index1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数为：\n",
      "11\n",
      "[3 3 1 3 3 1 2 3 1 3 2 3 1 2 1 3 2 2 1 1 3 2 3 3 2 3 3 3 3 2 3 3 1 2 3 1 1\n",
      " 1 1 2 2 3 2 2 3 1 3 3 2 1 1 1 2 1 2 3 3 3 1 1 1 3 2 3 3 1 2 1 1 2 3 1 2 1\n",
      " 3 2 3 1 2 1 2 2 3 1 1 3 2 1 2 3 3 1 3 1 2 2 1 1 2 3]\n",
      "迭代次数为：\n",
      "6\n",
      "[3. 3. 2. 1. 1. 3. 1. 1. 2. 3. 3. 2. 2. 1. 3. 1. 1. 2. 2. 3. 1. 2. 2. 3.\n",
      " 3. 2. 3. 1. 2. 2. 1. 1. 1. 2. 2. 1. 3. 3. 2. 3. 1. 3. 2. 2. 1. 3. 2. 3.\n",
      " 2. 1.]\n",
      "[3 2 3 1 1 3 1 1 3 2 2 3 3 1 2 1 1 3 3 2 1 3 3 3 2 3 3 1 3 3 1 1 1 3 3 1 2\n",
      " 3 3 2 1 2 3 3 1 3 3 2 3 1]\n",
      "[2 2 3 1 1 3 1 1 2 2 2 3 3 1 2 1 1 3 3 2 1 3 3 2 2 3 2 1 2 3 1 1 1 3 3 1 2\n",
      " 2 3 2 1 2 3 2 1 2 3 2 3 1]\n"
     ]
    }
   ],
   "source": [
    "iris_train = np.array(iris)[index1[:100]]\n",
    "iris_test = np.array(iris)[index1[100:]]\n",
    "#iris_x = iris[['sepal length in cm', 'sepal width in cm', 'petal length in cm', 'petal width in cm']]\n",
    "km_iris1 = Kmeans(np.array(iris_train[:, :-1]), 40, 5, 3, center_method = \"mean\")\n",
    "km_iris1.cal_all()\n",
    "train_mean = km_iris1.point_cluster_assign+1\n",
    "train_medoid = km_iris2.point_cluster_assign+1\n",
    "print(train_mean)\n",
    "km_iris2 = Kmeans(np.array(iris_train[:, :-1]), 40, 5, 3, center_method = \"medoid\")\n",
    "km_iris2.cal_all()\n",
    "print(iris_test[:,-1])\n",
    "test_mean = km_iris1.new_assign(iris_test[:, :-1])\n",
    "test_medoid = km_iris2.new_assign(iris_test[:, :-1])\n",
    "print(test_mean)\n",
    "print(test_medoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean[train_mean==3] = 4\n",
    "train_mean[train_mean==2] = 3\n",
    "train_mean[train_mean==4] = 2\n",
    "\n",
    "train_medoid[train_medoid==3] = 4\n",
    "train_medoid[train_medoid==2] = 3\n",
    "train_medoid[train_medoid==4] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris数据训练集——k-means聚类准确率为：\n",
      "0.89\n",
      "iris数据训练集——k-medoids聚类准确率为：\n",
      "0.88\n"
     ]
    }
   ],
   "source": [
    "accuracy_train_mean = len(np.array(range(len(iris_train)))[np.where(train_mean == np.array(iris_train[:, -1]))]) / len(iris_train)\n",
    "accuracy_train_medoid = len(np.array(range(len(iris_train)))[np.where(train_medoid == np.array(iris_train[:, -1]))]) / len(iris_train)\n",
    "print(\"iris数据训练集——k-means聚类准确率为：\")\n",
    "print(accuracy_train_mean)\n",
    "print(\"iris数据训练集——k-medoids聚类准确率为：\")\n",
    "print(accuracy_train_medoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean[test_mean==3] = 4\n",
    "test_mean[test_mean==2] = 3\n",
    "test_mean[test_mean==4] = 2\n",
    "\n",
    "test_medoid[test_medoid==3] = 4\n",
    "test_medoid[test_medoid==2] = 3\n",
    "test_medoid[test_medoid==4] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris数据测试集——k-means聚类准确率为：\n",
      "0.88\n",
      "iris数据测试集——k-medoids聚类准确率为：\n",
      "0.92\n"
     ]
    }
   ],
   "source": [
    "accuracy_test_mean = len(np.array(range(len(iris_test)))[np.where(test_mean == np.array(iris_test[:, -1]))]) / len(iris_test)\n",
    "accuracy_test_medoid = len(np.array(range(len(iris_test)))[np.where(test_medoid == np.array(iris_test[:, -1]))]) / len(iris_test)\n",
    "print(\"iris数据测试集——k-means聚类准确率为：\")\n",
    "print(accuracy_test_mean)\n",
    "print(\"iris数据测试集——k-medoids聚类准确率为：\")\n",
    "print(accuracy_test_medoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$可以看出，在训练集对样本进行聚类的结果中，k-means方法计算聚类中心的预测准确率要略高于k-medoids方法，而在利用训练集得出每个类别的中心后再对测试集进行类别预测的结果中，k-medoids方法的准确率则要高于k-means，这也符合这两个计算类别中心的方法的特点，k-medoids本身就是对于k-means算法对噪音敏感缺陷的一种改进，因此能更好地避免过拟合，即在测试集上表现更好，而在训练集上则是k-means拟合效果更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义Kmeans类与sklearn库中Kmeans方法准确度比较 $\\renewcommand{\\baselinestretch}{1.5}$\n",
    "本部分调用sklearn中关于聚类的库sklearn.cluster.KMeans来对上述Iris数据集进行测试，并与本报告中的自定义Kmeans类的预测准确率进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn在测试集上的准确率为：\n",
      "0.64\n",
      "sklearn在训练集上的准确率为：\n",
      "0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "np.random.seed(1)\n",
    "clf = KMeans(n_clusters=3, max_iter = 100)\n",
    "s = clf.fit(iris_train[:, :-1])\n",
    "train_predict = s.predict(iris_train[:, :-1])\n",
    "test_predict = s.predict(iris_test[:, :-1])\n",
    "accuracy_train_sklearn = len(np.array(range(len(iris_train)))[np.where(train_predict == np.array(iris_train[:, -1]))]) / len(iris_train)\n",
    "accuracy_test_sklearn = len(np.array(range(len(iris_test)))[np.where(test_predict == np.array(iris_test[:, -1]))]) / len(iris_test)\n",
    "print(\"sklearn在测试集上的准确率为：\")\n",
    "print(accuracy_train_sklearn)\n",
    "print(\"sklearn在训练集上的准确率为：\")\n",
    "print(accuracy_test_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>self_Kmeans</th>\n",
       "      <th>self_Kmedoids</th>\n",
       "      <th>sklearn_Kmeans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       self_Kmeans  self_Kmedoids  sklearn_Kmeans\n",
       "train         0.89           0.88            0.64\n",
       "test          0.88           0.92            0.68"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = pd.DataFrame({'self_Kmeans':[accuracy_train_mean,accuracy_test_mean],\n",
    "                         'self_Kmedoids':[accuracy_train_medoid, accuracy_test_medoid],\n",
    "                         'sklearn_Kmeans':[accuracy_train_sklearn, accuracy_test_sklearn]}).rename(index={0: 'train', 1: 'test'})\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$从以上表格中可以看出，在Iris数据集上，自定义的k-means和k-medoids算法在训练集和测试集预测准确度都是不错的，比sklearn库中提供的k-means聚类方法的预测准确度要高，这说明基于密度和距离的优化初始聚类中心的方法是有效的。并且由每次调用Kmeans类时输出的迭代次数可以看出，在样本数量在100-200之间（维度为3）时，迭代次数最大为16，目标函数值较快就达到了收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结 $\\renewcommand{\\baselinestretch}{1.5}$\n",
    "$\\quad$由于k-means算法易于描述，具有时间效率高且适于处理大规模数据等优点，因此被广泛地应用于然语言处理等多个领域。但k-means算法也有很多缺陷，如需要预先确定k值、会受到初始聚类中心影响、难以处理分类属性数据以及容易收敛于局部最优解等。目前已经有许多研究对这些问题提出了不同的解决办法。对于选择最优k值，有基于聚类有效性函数、遗传算法等解决方法；对于如何确定初始中心，有基于密度和优化算法等解决方法。本实验选取了其中的两篇论文来进行算法实现，并在模拟数据和真实数据上都得到了不错的聚类结果。\n",
    "\n",
    "$\\quad$本实验的亮点在于：\n",
    "\n",
    "（1）多处运用矩阵运算，例如在求初始数据对象两两之前距离以及在k-medoid方法计算聚类中心时，只进行一次矩阵运算（详见Kmeans类中的方法）,大大减少了重复计算次数以及for循环的使用数量，提高了运算效率；\n",
    "\n",
    "（2）改进k-means算法，选取被引次数较多的论文进行具体实现，在模拟数据和真实数据上都得到了不错的结果，相比于调用sklearn中Kmeans库的聚类结果准确度更高。\n",
    "\n",
    "$\\quad$本实验的不足之处在于：\n",
    "\n",
    "（1）仅仅选取了一个样本数量较少的数据集进行测试，对于大样本以及特征更多的数据集，自定义Kmeans类的聚类效果还有待检验；\n",
    "\n",
    "（2）自定义Kmeans类中有一些参数，如 minpts（用以xi为中心，包含常数minpts个数据对象的半径来衡量xi的密度）和 p（初始中心的备选点的个数）没有进行调优；\n",
    "\n",
    "（3）自定义Kmeans类虽然对k-means算法进行了改进并提供了两种计算聚类中心的方法，但是整体上提供的方法以及参数的设定上还比较单一，可以进一步对其进行丰富。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "636.8px",
    "left": "1372.6px",
    "top": "75px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
